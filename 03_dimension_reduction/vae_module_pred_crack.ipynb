{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f214ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "X_train = []\n",
    "data_path = 'C:/Users/Administrator/Desktop/lstm_result0605/Dataset1_sample'\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset_noslice_sample'\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_train'\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_train_cut'\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_cut_resize'\n",
    "folder_list = os.listdir(data_path)\n",
    "for folder in folder_list:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    file_list = os.listdir(folder_path)\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        img = cv2.imread(file_path)\n",
    "        pic = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        pic_list = pic.tolist()\n",
    "        X_train.append(pic_list)\n",
    "X_train_np = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4807db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7296, 92, 136)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f3ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a path of crack.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c508225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Sampling.call of <__main__.Sampling object at 0x0000020656E88820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Sampling.call of <__main__.Sampling object at 0x0000020656E88820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 92, 136, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 46, 68, 32)   320         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 23, 34, 64)   18496       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50048)        0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          10009800    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 100)          20100       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 100)          20100       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling_1 (Sampling)           (None, 100)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,068,816\n",
      "Trainable params: 10,068,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(92, 136, 1))\n",
    "#encoder_inputs = keras.Input(shape=(56, 100, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(200, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7609015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50048)             5054848   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 23, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 46, 68, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 92, 136, 32)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 92, 136, 1)        289       \n",
      "=================================================================\n",
      "Total params: 5,110,529\n",
      "Trainable params: 5,110,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(23*34*64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((23, 34, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00efb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b530613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.expand_dims(X_train_np, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dffd8779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7296, 92, 136, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abbe9df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "57/57 [==============================] - 65s 1s/step - loss: 3720.2108 - reconstruction_loss: 1333.0515 - kl_loss: 121.8585\n",
      "Epoch 2/300\n",
      "57/57 [==============================] - 66s 1s/step - loss: 161.4443 - reconstruction_loss: 147.1633 - kl_loss: 10.8369\n",
      "Epoch 3/300\n",
      "57/57 [==============================] - 66s 1s/step - loss: 150.7994 - reconstruction_loss: 140.7818 - kl_loss: 8.9400\n",
      "Epoch 4/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 146.9154 - reconstruction_loss: 137.7200 - kl_loss: 8.7695\n",
      "Epoch 5/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 144.2584 - reconstruction_loss: 134.7388 - kl_loss: 8.5791\n",
      "Epoch 6/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 141.6928 - reconstruction_loss: 132.8765 - kl_loss: 8.2379\n",
      "Epoch 7/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 137.9095 - reconstruction_loss: 132.0652 - kl_loss: 6.6893\n",
      "Epoch 8/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 137.5497 - reconstruction_loss: 131.4913 - kl_loss: 6.3576\n",
      "Epoch 9/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 138.0032 - reconstruction_loss: 130.9745 - kl_loss: 5.9442\n",
      "Epoch 10/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 135.7121 - reconstruction_loss: 130.9960 - kl_loss: 5.5245\n",
      "Epoch 11/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 136.8513 - reconstruction_loss: 130.5603 - kl_loss: 5.4958\n",
      "Epoch 12/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 136.7090 - reconstruction_loss: 130.6539 - kl_loss: 5.6467\n",
      "Epoch 13/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 136.7768 - reconstruction_loss: 130.6236 - kl_loss: 5.1494\n",
      "Epoch 14/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 133.9168 - reconstruction_loss: 130.1590 - kl_loss: 4.9860\n",
      "Epoch 15/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 134.0961 - reconstruction_loss: 130.1164 - kl_loss: 4.6124\n",
      "Epoch 16/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 135.0592 - reconstruction_loss: 129.8428 - kl_loss: 4.3554\n",
      "Epoch 17/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 133.0936 - reconstruction_loss: 129.9377 - kl_loss: 4.0890\n",
      "Epoch 18/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 133.5923 - reconstruction_loss: 129.7551 - kl_loss: 3.7312\n",
      "Epoch 19/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 133.1079 - reconstruction_loss: 129.0968 - kl_loss: 3.4740\n",
      "Epoch 20/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 131.1808 - reconstruction_loss: 126.4946 - kl_loss: 3.6146\n",
      "Epoch 21/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 126.3572 - reconstruction_loss: 120.9587 - kl_loss: 4.4861\n",
      "Epoch 22/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 124.6094 - reconstruction_loss: 119.4168 - kl_loss: 4.8858\n",
      "Epoch 23/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 121.7788 - reconstruction_loss: 118.5379 - kl_loss: 4.5033\n",
      "Epoch 24/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 123.4739 - reconstruction_loss: 117.9121 - kl_loss: 4.4415\n",
      "Epoch 25/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 121.9738 - reconstruction_loss: 117.2543 - kl_loss: 4.3451\n",
      "Epoch 26/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 121.7228 - reconstruction_loss: 116.4576 - kl_loss: 4.2366\n",
      "Epoch 27/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 120.3312 - reconstruction_loss: 115.3924 - kl_loss: 4.3763\n",
      "Epoch 28/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 119.3341 - reconstruction_loss: 114.3007 - kl_loss: 4.2957\n",
      "Epoch 29/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 117.0304 - reconstruction_loss: 110.5900 - kl_loss: 4.9722\n",
      "Epoch 30/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 113.7826 - reconstruction_loss: 107.2379 - kl_loss: 5.4897\n",
      "Epoch 31/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 111.1951 - reconstruction_loss: 105.9284 - kl_loss: 5.6651\n",
      "Epoch 32/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 110.6570 - reconstruction_loss: 104.8907 - kl_loss: 5.8286\n",
      "Epoch 33/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 108.5709 - reconstruction_loss: 103.2433 - kl_loss: 6.2060\n",
      "Epoch 34/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 108.4782 - reconstruction_loss: 102.4168 - kl_loss: 6.1588\n",
      "Epoch 35/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 108.1607 - reconstruction_loss: 100.9261 - kl_loss: 6.4797\n",
      "Epoch 36/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 106.2855 - reconstruction_loss: 100.3099 - kl_loss: 6.5274\n",
      "Epoch 37/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 107.3791 - reconstruction_loss: 100.3929 - kl_loss: 6.2829\n",
      "Epoch 38/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 106.0760 - reconstruction_loss: 99.4019 - kl_loss: 6.5781\n",
      "Epoch 39/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 105.8591 - reconstruction_loss: 98.8271 - kl_loss: 6.6926\n",
      "Epoch 40/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 104.0919 - reconstruction_loss: 97.9816 - kl_loss: 6.7024\n",
      "Epoch 41/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 102.9763 - reconstruction_loss: 97.1333 - kl_loss: 6.8714\n",
      "Epoch 42/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 103.5052 - reconstruction_loss: 96.3731 - kl_loss: 6.9389\n",
      "Epoch 43/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 102.1770 - reconstruction_loss: 95.4307 - kl_loss: 6.9247\n",
      "Epoch 44/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 102.0047 - reconstruction_loss: 94.4480 - kl_loss: 7.1640\n",
      "Epoch 45/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 102.5095 - reconstruction_loss: 93.5951 - kl_loss: 7.1365\n",
      "Epoch 46/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 100.9220 - reconstruction_loss: 92.2640 - kl_loss: 7.2968\n",
      "Epoch 47/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 100.0256 - reconstruction_loss: 91.3264 - kl_loss: 7.3862\n",
      "Epoch 48/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 97.7789 - reconstruction_loss: 90.4219 - kl_loss: 7.4900\n",
      "Epoch 49/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 97.1561 - reconstruction_loss: 90.0807 - kl_loss: 7.3372\n",
      "Epoch 50/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 97.0264 - reconstruction_loss: 89.0324 - kl_loss: 7.4514\n",
      "Epoch 51/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 96.4012 - reconstruction_loss: 88.5082 - kl_loss: 7.4831\n",
      "Epoch 52/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 95.4837 - reconstruction_loss: 88.2202 - kl_loss: 7.4020\n",
      "Epoch 53/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 96.1709 - reconstruction_loss: 88.0998 - kl_loss: 7.2953\n",
      "Epoch 54/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 94.7418 - reconstruction_loss: 87.4152 - kl_loss: 7.5504\n",
      "Epoch 55/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 94.2448 - reconstruction_loss: 87.0730 - kl_loss: 7.5370\n",
      "Epoch 56/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 93.8857 - reconstruction_loss: 86.7830 - kl_loss: 7.5493\n",
      "Epoch 57/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 94.2568 - reconstruction_loss: 87.9499 - kl_loss: 7.0458\n",
      "Epoch 58/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 93.8900 - reconstruction_loss: 86.4369 - kl_loss: 7.5357\n",
      "Epoch 59/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 94.2009 - reconstruction_loss: 85.8507 - kl_loss: 7.7116\n",
      "Epoch 60/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 92.2538 - reconstruction_loss: 85.5720 - kl_loss: 7.7172\n",
      "Epoch 61/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 93.0395 - reconstruction_loss: 85.1366 - kl_loss: 7.7603\n",
      "Epoch 62/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 91.9151 - reconstruction_loss: 84.7298 - kl_loss: 7.7606\n",
      "Epoch 63/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 92.6651 - reconstruction_loss: 84.5247 - kl_loss: 7.7491\n",
      "Epoch 64/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 92.0817 - reconstruction_loss: 84.1291 - kl_loss: 7.8397\n",
      "Epoch 65/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 91.0304 - reconstruction_loss: 83.9023 - kl_loss: 7.9021\n",
      "Epoch 66/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 91.0820 - reconstruction_loss: 83.6271 - kl_loss: 7.8768\n",
      "Epoch 67/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 92.1362 - reconstruction_loss: 83.3258 - kl_loss: 7.9259\n",
      "Epoch 68/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 90.4433 - reconstruction_loss: 83.0094 - kl_loss: 7.9812\n",
      "Epoch 69/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 91.7092 - reconstruction_loss: 82.9481 - kl_loss: 7.9011\n",
      "Epoch 70/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 90.2144 - reconstruction_loss: 82.5198 - kl_loss: 8.0162\n",
      "Epoch 71/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 90.4753 - reconstruction_loss: 82.6634 - kl_loss: 7.7936\n",
      "Epoch 72/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 89.4597 - reconstruction_loss: 82.2727 - kl_loss: 7.9106\n",
      "Epoch 73/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 89.8701 - reconstruction_loss: 81.8432 - kl_loss: 7.9742\n",
      "Epoch 74/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 89.9212 - reconstruction_loss: 81.7132 - kl_loss: 8.0001\n",
      "Epoch 75/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.4429 - reconstruction_loss: 81.5586 - kl_loss: 7.9812\n",
      "Epoch 76/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 90.6417 - reconstruction_loss: 81.3783 - kl_loss: 8.0242\n",
      "Epoch 77/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 88.6593 - reconstruction_loss: 81.2505 - kl_loss: 7.9343\n",
      "Epoch 78/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 89.2247 - reconstruction_loss: 81.1746 - kl_loss: 7.9302\n",
      "Epoch 79/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 89.0922 - reconstruction_loss: 81.0171 - kl_loss: 8.0178\n",
      "Epoch 80/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.9272 - reconstruction_loss: 80.8712 - kl_loss: 8.0641\n",
      "Epoch 81/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 89.5149 - reconstruction_loss: 80.4935 - kl_loss: 7.9951\n",
      "Epoch 82/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.8172 - reconstruction_loss: 80.5034 - kl_loss: 7.9932\n",
      "Epoch 83/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.4128 - reconstruction_loss: 80.4763 - kl_loss: 7.9420\n",
      "Epoch 84/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.6543 - reconstruction_loss: 80.4975 - kl_loss: 7.9066\n",
      "Epoch 85/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.4145 - reconstruction_loss: 80.2389 - kl_loss: 8.0426\n",
      "Epoch 86/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.0369 - reconstruction_loss: 80.0022 - kl_loss: 8.0427\n",
      "Epoch 87/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.2756 - reconstruction_loss: 80.1002 - kl_loss: 7.9926\n",
      "Epoch 88/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 88.4619 - reconstruction_loss: 79.8672 - kl_loss: 8.0510\n",
      "Epoch 89/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.9122 - reconstruction_loss: 79.8257 - kl_loss: 8.0600\n",
      "Epoch 90/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.1802 - reconstruction_loss: 79.7282 - kl_loss: 8.0672\n",
      "Epoch 91/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.3264 - reconstruction_loss: 79.7505 - kl_loss: 8.0206\n",
      "Epoch 92/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.3798 - reconstruction_loss: 79.5536 - kl_loss: 8.0290\n",
      "Epoch 93/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.9325 - reconstruction_loss: 79.4780 - kl_loss: 8.0662\n",
      "Epoch 94/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.2538 - reconstruction_loss: 79.2762 - kl_loss: 8.0287\n",
      "Epoch 95/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.8649 - reconstruction_loss: 79.2481 - kl_loss: 8.0180\n",
      "Epoch 96/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.0240 - reconstruction_loss: 79.1894 - kl_loss: 8.0377\n",
      "Epoch 97/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.4742 - reconstruction_loss: 79.2446 - kl_loss: 8.0029\n",
      "Epoch 98/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.5154 - reconstruction_loss: 79.0860 - kl_loss: 8.0815\n",
      "Epoch 99/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.5028 - reconstruction_loss: 78.8732 - kl_loss: 8.0979\n",
      "Epoch 100/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.3882 - reconstruction_loss: 79.0286 - kl_loss: 8.0417\n",
      "Epoch 101/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.7272 - reconstruction_loss: 78.9174 - kl_loss: 8.0673\n",
      "Epoch 102/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 87.3017 - reconstruction_loss: 78.6588 - kl_loss: 8.1092\n",
      "Epoch 103/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.5667 - reconstruction_loss: 78.6884 - kl_loss: 8.0467\n",
      "Epoch 104/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.4335 - reconstruction_loss: 78.8950 - kl_loss: 8.0074\n",
      "Epoch 105/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 88.7352 - reconstruction_loss: 79.5707 - kl_loss: 7.9129\n",
      "Epoch 106/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.5134 - reconstruction_loss: 78.5318 - kl_loss: 8.1096\n",
      "Epoch 107/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.2353 - reconstruction_loss: 78.4811 - kl_loss: 8.1093\n",
      "Epoch 108/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.2394 - reconstruction_loss: 78.4268 - kl_loss: 8.1173\n",
      "Epoch 109/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.1825 - reconstruction_loss: 78.3654 - kl_loss: 8.0377\n",
      "Epoch 110/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.0372 - reconstruction_loss: 78.2790 - kl_loss: 8.1652\n",
      "Epoch 111/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.3289 - reconstruction_loss: 78.2373 - kl_loss: 8.1081\n",
      "Epoch 112/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 87.1282 - reconstruction_loss: 78.0794 - kl_loss: 8.1413\n",
      "Epoch 113/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.4123 - reconstruction_loss: 78.1828 - kl_loss: 8.1039\n",
      "Epoch 114/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.6321 - reconstruction_loss: 78.0764 - kl_loss: 8.1255\n",
      "Epoch 115/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.9897 - reconstruction_loss: 78.0171 - kl_loss: 8.1755\n",
      "Epoch 116/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.7009 - reconstruction_loss: 77.8776 - kl_loss: 8.1844\n",
      "Epoch 117/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.4061 - reconstruction_loss: 77.7953 - kl_loss: 8.1163\n",
      "Epoch 118/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.6648 - reconstruction_loss: 77.8387 - kl_loss: 8.1865\n",
      "Epoch 119/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.6181 - reconstruction_loss: 77.7520 - kl_loss: 8.1136\n",
      "Epoch 120/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.1702 - reconstruction_loss: 77.6616 - kl_loss: 8.1191\n",
      "Epoch 121/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.8214 - reconstruction_loss: 77.6289 - kl_loss: 8.1718\n",
      "Epoch 122/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 85.0773 - reconstruction_loss: 77.7397 - kl_loss: 8.0991\n",
      "Epoch 123/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.6588 - reconstruction_loss: 77.6691 - kl_loss: 8.1155\n",
      "Epoch 124/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 69s 1s/step - loss: 85.5397 - reconstruction_loss: 77.7257 - kl_loss: 8.1632\n",
      "Epoch 125/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.0680 - reconstruction_loss: 77.4682 - kl_loss: 8.1143\n",
      "Epoch 126/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.6904 - reconstruction_loss: 77.4266 - kl_loss: 8.1172\n",
      "Epoch 127/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.9910 - reconstruction_loss: 77.4267 - kl_loss: 8.1639\n",
      "Epoch 128/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.0819 - reconstruction_loss: 77.4663 - kl_loss: 8.1063\n",
      "Epoch 129/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.6013 - reconstruction_loss: 77.2755 - kl_loss: 8.2133\n",
      "Epoch 130/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.1920 - reconstruction_loss: 77.2751 - kl_loss: 8.1414\n",
      "Epoch 131/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.1689 - reconstruction_loss: 77.2206 - kl_loss: 8.1944\n",
      "Epoch 132/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.0871 - reconstruction_loss: 77.3655 - kl_loss: 8.1188\n",
      "Epoch 133/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.1585 - reconstruction_loss: 77.4522 - kl_loss: 8.1417\n",
      "Epoch 134/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.4306 - reconstruction_loss: 77.9573 - kl_loss: 7.9866\n",
      "Epoch 135/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 86.6988 - reconstruction_loss: 77.3450 - kl_loss: 8.1675\n",
      "Epoch 136/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 85.0585 - reconstruction_loss: 77.0653 - kl_loss: 8.2274\n",
      "Epoch 137/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.0284 - reconstruction_loss: 77.0830 - kl_loss: 8.2680\n",
      "Epoch 138/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.7473 - reconstruction_loss: 76.8844 - kl_loss: 8.2309\n",
      "Epoch 139/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.7377 - reconstruction_loss: 76.8846 - kl_loss: 8.2257\n",
      "Epoch 140/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.2115 - reconstruction_loss: 76.7645 - kl_loss: 8.1787\n",
      "Epoch 141/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.1004 - reconstruction_loss: 78.4537 - kl_loss: 7.9665\n",
      "Epoch 142/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 86.7763 - reconstruction_loss: 78.7847 - kl_loss: 8.1218\n",
      "Epoch 143/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.8727 - reconstruction_loss: 76.9021 - kl_loss: 8.1996\n",
      "Epoch 144/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.0648 - reconstruction_loss: 76.7090 - kl_loss: 8.2757\n",
      "Epoch 145/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.6700 - reconstruction_loss: 76.7002 - kl_loss: 8.2722\n",
      "Epoch 146/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.5343 - reconstruction_loss: 76.8747 - kl_loss: 8.2233\n",
      "Epoch 147/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.4065 - reconstruction_loss: 76.4551 - kl_loss: 8.2726\n",
      "Epoch 148/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.8138 - reconstruction_loss: 76.4343 - kl_loss: 8.2933\n",
      "Epoch 149/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.3854 - reconstruction_loss: 76.4858 - kl_loss: 8.2308\n",
      "Epoch 150/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.1179 - reconstruction_loss: 76.3805 - kl_loss: 8.2762\n",
      "Epoch 151/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.4476 - reconstruction_loss: 76.3549 - kl_loss: 8.2853\n",
      "Epoch 152/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.3832 - reconstruction_loss: 76.2936 - kl_loss: 8.2535\n",
      "Epoch 153/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.4968 - reconstruction_loss: 76.2122 - kl_loss: 8.2783\n",
      "Epoch 154/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 85.0691 - reconstruction_loss: 76.2857 - kl_loss: 8.2606\n",
      "Epoch 155/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.9805 - reconstruction_loss: 76.2181 - kl_loss: 8.3212\n",
      "Epoch 156/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 84.2543 - reconstruction_loss: 76.1434 - kl_loss: 8.3264\n",
      "Epoch 157/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 85.2506 - reconstruction_loss: 76.1452 - kl_loss: 8.3061\n",
      "Epoch 158/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.0555 - reconstruction_loss: 76.1309 - kl_loss: 8.3092\n",
      "Epoch 159/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.5162 - reconstruction_loss: 76.0210 - kl_loss: 8.3445\n",
      "Epoch 160/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.3615 - reconstruction_loss: 75.8900 - kl_loss: 8.3552\n",
      "Epoch 161/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.1292 - reconstruction_loss: 75.9637 - kl_loss: 8.3675\n",
      "Epoch 162/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.7517 - reconstruction_loss: 75.8802 - kl_loss: 8.3272\n",
      "Epoch 163/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.4682 - reconstruction_loss: 75.9350 - kl_loss: 8.3392\n",
      "Epoch 164/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.6723 - reconstruction_loss: 75.8223 - kl_loss: 8.3499\n",
      "Epoch 165/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.2493 - reconstruction_loss: 75.7631 - kl_loss: 8.3731\n",
      "Epoch 166/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.4312 - reconstruction_loss: 75.7067 - kl_loss: 8.3167\n",
      "Epoch 167/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.3377 - reconstruction_loss: 75.6556 - kl_loss: 8.3862\n",
      "Epoch 168/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.4413 - reconstruction_loss: 75.6566 - kl_loss: 8.2879\n",
      "Epoch 169/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.7803 - reconstruction_loss: 75.7341 - kl_loss: 8.3467\n",
      "Epoch 170/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.6233 - reconstruction_loss: 75.5495 - kl_loss: 8.3267\n",
      "Epoch 171/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.9635 - reconstruction_loss: 75.5742 - kl_loss: 8.3318\n",
      "Epoch 172/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.0225 - reconstruction_loss: 75.6439 - kl_loss: 8.3845\n",
      "Epoch 173/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 84.4738 - reconstruction_loss: 75.6176 - kl_loss: 8.3483\n",
      "Epoch 174/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 84.7804 - reconstruction_loss: 75.5798 - kl_loss: 8.3623\n",
      "Epoch 175/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.7436 - reconstruction_loss: 75.6761 - kl_loss: 8.3298\n",
      "Epoch 176/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.6632 - reconstruction_loss: 75.4539 - kl_loss: 8.3744\n",
      "Epoch 177/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.4838 - reconstruction_loss: 75.3642 - kl_loss: 8.3639\n",
      "Epoch 178/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 84.0040 - reconstruction_loss: 75.6151 - kl_loss: 8.3061\n",
      "Epoch 179/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.5001 - reconstruction_loss: 75.4783 - kl_loss: 8.3429\n",
      "Epoch 180/300\n",
      "57/57 [==============================] - 75s 1s/step - loss: 83.0917 - reconstruction_loss: 75.3275 - kl_loss: 8.3914\n",
      "Epoch 181/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 83.8038 - reconstruction_loss: 75.3321 - kl_loss: 8.3846\n",
      "Epoch 182/300\n",
      "57/57 [==============================] - 72s 1s/step - loss: 84.3123 - reconstruction_loss: 75.2547 - kl_loss: 8.3841\n",
      "Epoch 183/300\n",
      "57/57 [==============================] - 78s 1s/step - loss: 83.1504 - reconstruction_loss: 75.2009 - kl_loss: 8.3524\n",
      "Epoch 184/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 83.6772 - reconstruction_loss: 75.3034 - kl_loss: 8.3830\n",
      "Epoch 185/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.2914 - reconstruction_loss: 75.1348 - kl_loss: 8.4048\n",
      "Epoch 186/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.5167 - reconstruction_loss: 75.1332 - kl_loss: 8.3983\n",
      "Epoch 187/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.6405 - reconstruction_loss: 75.1524 - kl_loss: 8.3942\n",
      "Epoch 188/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.7385 - reconstruction_loss: 75.2374 - kl_loss: 8.3890\n",
      "Epoch 189/300\n",
      "57/57 [==============================] - 76s 1s/step - loss: 83.7844 - reconstruction_loss: 75.6420 - kl_loss: 8.4402\n",
      "Epoch 190/300\n",
      "57/57 [==============================] - 72s 1s/step - loss: 93.2118 - reconstruction_loss: 82.0179 - kl_loss: 7.5746\n",
      "Epoch 191/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 84.7950 - reconstruction_loss: 76.0730 - kl_loss: 8.3359\n",
      "Epoch 192/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 84.6029 - reconstruction_loss: 75.5428 - kl_loss: 8.4001\n",
      "Epoch 193/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.6715 - reconstruction_loss: 75.3271 - kl_loss: 8.3432\n",
      "Epoch 194/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.4364 - reconstruction_loss: 75.2280 - kl_loss: 8.3724\n",
      "Epoch 195/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.5978 - reconstruction_loss: 75.1062 - kl_loss: 8.4007\n",
      "Epoch 196/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.5158 - reconstruction_loss: 75.0976 - kl_loss: 8.4070\n",
      "Epoch 197/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 84.0183 - reconstruction_loss: 75.0481 - kl_loss: 8.4110\n",
      "Epoch 198/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 82.8671 - reconstruction_loss: 74.9420 - kl_loss: 8.4377\n",
      "Epoch 199/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 82.8499 - reconstruction_loss: 74.9661 - kl_loss: 8.3740\n",
      "Epoch 200/300\n",
      "57/57 [==============================] - 72s 1s/step - loss: 82.6320 - reconstruction_loss: 74.8907 - kl_loss: 8.4216\n",
      "Epoch 201/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 82.2703 - reconstruction_loss: 74.8719 - kl_loss: 8.4053\n",
      "Epoch 202/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 82.9705 - reconstruction_loss: 74.9093 - kl_loss: 8.3698\n",
      "Epoch 203/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.6906 - reconstruction_loss: 74.8294 - kl_loss: 8.3780\n",
      "Epoch 204/300\n",
      "57/57 [==============================] - 74s 1s/step - loss: 83.2938 - reconstruction_loss: 74.7319 - kl_loss: 8.4397\n",
      "Epoch 205/300\n",
      "57/57 [==============================] - 79s 1s/step - loss: 83.8351 - reconstruction_loss: 74.9644 - kl_loss: 8.3880\n",
      "Epoch 206/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 83.2768 - reconstruction_loss: 74.7857 - kl_loss: 8.4701\n",
      "Epoch 207/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 82.8706 - reconstruction_loss: 74.7464 - kl_loss: 8.4264\n",
      "Epoch 208/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.1595 - reconstruction_loss: 74.8174 - kl_loss: 8.4767\n",
      "Epoch 209/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.9167 - reconstruction_loss: 74.6618 - kl_loss: 8.4395\n",
      "Epoch 210/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 83.1048 - reconstruction_loss: 74.7127 - kl_loss: 8.4777\n",
      "Epoch 211/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.5385 - reconstruction_loss: 74.6877 - kl_loss: 8.3957\n",
      "Epoch 212/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 83.2220 - reconstruction_loss: 74.6472 - kl_loss: 8.3868\n",
      "Epoch 213/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 83.2315 - reconstruction_loss: 74.6106 - kl_loss: 8.4579\n",
      "Epoch 214/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.0925 - reconstruction_loss: 74.5782 - kl_loss: 8.4195\n",
      "Epoch 215/300\n",
      "57/57 [==============================] - 66s 1s/step - loss: 83.8398 - reconstruction_loss: 74.4987 - kl_loss: 8.4872\n",
      "Epoch 216/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.2219 - reconstruction_loss: 74.5093 - kl_loss: 8.4916\n",
      "Epoch 217/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.7218 - reconstruction_loss: 74.5785 - kl_loss: 8.4343\n",
      "Epoch 218/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.8399 - reconstruction_loss: 74.4709 - kl_loss: 8.4608\n",
      "Epoch 219/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.0240 - reconstruction_loss: 74.4780 - kl_loss: 8.4774\n",
      "Epoch 220/300\n",
      "57/57 [==============================] - 66s 1s/step - loss: 83.3427 - reconstruction_loss: 74.4942 - kl_loss: 8.4426\n",
      "Epoch 221/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.5527 - reconstruction_loss: 74.4849 - kl_loss: 8.3995\n",
      "Epoch 222/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.0051 - reconstruction_loss: 74.5116 - kl_loss: 8.3901\n",
      "Epoch 223/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 81.8193 - reconstruction_loss: 74.4528 - kl_loss: 8.4546\n",
      "Epoch 224/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.2465 - reconstruction_loss: 74.4292 - kl_loss: 8.4430\n",
      "Epoch 225/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.7504 - reconstruction_loss: 74.4137 - kl_loss: 8.4718\n",
      "Epoch 226/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 83.3280 - reconstruction_loss: 74.3949 - kl_loss: 8.4178\n",
      "Epoch 227/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 81.8926 - reconstruction_loss: 74.3146 - kl_loss: 8.4117\n",
      "Epoch 228/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.7475 - reconstruction_loss: 74.3795 - kl_loss: 8.4555\n",
      "Epoch 229/300\n",
      "57/57 [==============================] - 67s 1s/step - loss: 83.2741 - reconstruction_loss: 74.4251 - kl_loss: 8.4087\n",
      "Epoch 230/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 82.3610 - reconstruction_loss: 74.3705 - kl_loss: 8.4507\n",
      "Epoch 231/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 83.5001 - reconstruction_loss: 74.2711 - kl_loss: 8.4937\n",
      "Epoch 232/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.4790 - reconstruction_loss: 74.2517 - kl_loss: 8.4206\n",
      "Epoch 233/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 82.6637 - reconstruction_loss: 74.4790 - kl_loss: 8.4170\n",
      "Epoch 234/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 90.2874 - reconstruction_loss: 82.7766 - kl_loss: 7.7581\n",
      "Epoch 235/300\n",
      "57/57 [==============================] - 72s 1s/step - loss: 84.5014 - reconstruction_loss: 75.8094 - kl_loss: 8.3237\n",
      "Epoch 236/300\n",
      "57/57 [==============================] - 73s 1s/step - loss: 84.3827 - reconstruction_loss: 74.9824 - kl_loss: 8.3946\n",
      "Epoch 237/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 83.1486 - reconstruction_loss: 74.6130 - kl_loss: 8.4604\n",
      "Epoch 238/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.0395 - reconstruction_loss: 74.5558 - kl_loss: 8.4532\n",
      "Epoch 239/300\n",
      "57/57 [==============================] - 80s 1s/step - loss: 83.2378 - reconstruction_loss: 74.3960 - kl_loss: 8.4203\n",
      "Epoch 240/300\n",
      "57/57 [==============================] - 87s 2s/step - loss: 83.3340 - reconstruction_loss: 74.2698 - kl_loss: 8.4148\n",
      "Epoch 241/300\n",
      "57/57 [==============================] - 73s 1s/step - loss: 82.3991 - reconstruction_loss: 74.3478 - kl_loss: 8.4725\n",
      "Epoch 242/300\n",
      "57/57 [==============================] - 74s 1s/step - loss: 82.6191 - reconstruction_loss: 74.3074 - kl_loss: 8.5068\n",
      "Epoch 243/300\n",
      "57/57 [==============================] - 77s 1s/step - loss: 82.0369 - reconstruction_loss: 74.1806 - kl_loss: 8.4686\n",
      "Epoch 244/300\n",
      "57/57 [==============================] - 75s 1s/step - loss: 82.6358 - reconstruction_loss: 74.2587 - kl_loss: 8.4559\n",
      "Epoch 245/300\n",
      "57/57 [==============================] - 78s 1s/step - loss: 82.7630 - reconstruction_loss: 74.2143 - kl_loss: 8.4386\n",
      "Epoch 246/300\n",
      "57/57 [==============================] - 76s 1s/step - loss: 82.1578 - reconstruction_loss: 74.1598 - kl_loss: 8.4270\n",
      "Epoch 247/300\n",
      "57/57 [==============================] - 75s 1s/step - loss: 83.1494 - reconstruction_loss: 74.1326 - kl_loss: 8.4566\n",
      "Epoch 248/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 69s 1s/step - loss: 82.7037 - reconstruction_loss: 74.0943 - kl_loss: 8.4614\n",
      "Epoch 249/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.7565 - reconstruction_loss: 74.0746 - kl_loss: 8.3768\n",
      "Epoch 250/300\n",
      "57/57 [==============================] - 83s 1s/step - loss: 83.2030 - reconstruction_loss: 74.0894 - kl_loss: 8.4803\n",
      "Epoch 251/300\n",
      "57/57 [==============================] - 113s 2s/step - loss: 82.8307 - reconstruction_loss: 74.0798 - kl_loss: 8.4966\n",
      "Epoch 252/300\n",
      "57/57 [==============================] - 115s 2s/step - loss: 82.4125 - reconstruction_loss: 74.0706 - kl_loss: 8.4764\n",
      "Epoch 253/300\n",
      "57/57 [==============================] - 117s 2s/step - loss: 82.3925 - reconstruction_loss: 74.0309 - kl_loss: 8.4793\n",
      "Epoch 254/300\n",
      "57/57 [==============================] - 117s 2s/step - loss: 82.7576 - reconstruction_loss: 74.0617 - kl_loss: 8.4651\n",
      "Epoch 255/300\n",
      "57/57 [==============================] - 117s 2s/step - loss: 82.6929 - reconstruction_loss: 74.0465 - kl_loss: 8.4257\n",
      "Epoch 256/300\n",
      "57/57 [==============================] - 118s 2s/step - loss: 82.9798 - reconstruction_loss: 73.9764 - kl_loss: 8.4498\n",
      "Epoch 257/300\n",
      "57/57 [==============================] - 82s 1s/step - loss: 81.5019 - reconstruction_loss: 74.0088 - kl_loss: 8.4832\n",
      "Epoch 258/300\n",
      "57/57 [==============================] - 73s 1s/step - loss: 82.3149 - reconstruction_loss: 74.0255 - kl_loss: 8.5076\n",
      "Epoch 259/300\n",
      "57/57 [==============================] - 78s 1s/step - loss: 82.3620 - reconstruction_loss: 73.9424 - kl_loss: 8.4246\n",
      "Epoch 260/300\n",
      "57/57 [==============================] - 77s 1s/step - loss: 82.7848 - reconstruction_loss: 73.9528 - kl_loss: 8.4643\n",
      "Epoch 261/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 82.6074 - reconstruction_loss: 73.9833 - kl_loss: 8.5229\n",
      "Epoch 262/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 82.0235 - reconstruction_loss: 73.9307 - kl_loss: 8.4423\n",
      "Epoch 263/300\n",
      "57/57 [==============================] - 68s 1s/step - loss: 81.9404 - reconstruction_loss: 73.9966 - kl_loss: 8.4260\n",
      "Epoch 264/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.7209 - reconstruction_loss: 73.9250 - kl_loss: 8.4754\n",
      "Epoch 265/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.7996 - reconstruction_loss: 73.8475 - kl_loss: 8.4592\n",
      "Epoch 266/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.1074 - reconstruction_loss: 73.8813 - kl_loss: 8.4595\n",
      "Epoch 267/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.9976 - reconstruction_loss: 73.8145 - kl_loss: 8.4705\n",
      "Epoch 268/300\n",
      "57/57 [==============================] - 72s 1s/step - loss: 82.5814 - reconstruction_loss: 73.7746 - kl_loss: 8.4809\n",
      "Epoch 269/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 82.3245 - reconstruction_loss: 73.8111 - kl_loss: 8.4606\n",
      "Epoch 270/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 82.5568 - reconstruction_loss: 73.8374 - kl_loss: 8.4450\n",
      "Epoch 271/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.2307 - reconstruction_loss: 73.8736 - kl_loss: 8.4125\n",
      "Epoch 272/300\n",
      "57/57 [==============================] - 72s 1s/step - loss: 82.0378 - reconstruction_loss: 73.8443 - kl_loss: 8.4507\n",
      "Epoch 273/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.0469 - reconstruction_loss: 73.8007 - kl_loss: 8.4767\n",
      "Epoch 274/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 81.8139 - reconstruction_loss: 73.7184 - kl_loss: 8.4251\n",
      "Epoch 275/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.5282 - reconstruction_loss: 73.7543 - kl_loss: 8.4678\n",
      "Epoch 276/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.5252 - reconstruction_loss: 73.7343 - kl_loss: 8.4754\n",
      "Epoch 277/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.1486 - reconstruction_loss: 73.7119 - kl_loss: 8.4777\n",
      "Epoch 278/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.2078 - reconstruction_loss: 73.7343 - kl_loss: 8.4564\n",
      "Epoch 279/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 82.2971 - reconstruction_loss: 73.5892 - kl_loss: 8.5210\n",
      "Epoch 280/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 81.1468 - reconstruction_loss: 73.8138 - kl_loss: 8.4616\n",
      "Epoch 281/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.1383 - reconstruction_loss: 73.7225 - kl_loss: 8.4738\n",
      "Epoch 282/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.2047 - reconstruction_loss: 73.6194 - kl_loss: 8.4638\n",
      "Epoch 283/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 83.3850 - reconstruction_loss: 73.6556 - kl_loss: 8.4633\n",
      "Epoch 284/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.7590 - reconstruction_loss: 73.6726 - kl_loss: 8.4553\n",
      "Epoch 285/300\n",
      "57/57 [==============================] - 70s 1s/step - loss: 81.4928 - reconstruction_loss: 73.5787 - kl_loss: 8.4669\n",
      "Epoch 286/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 82.7455 - reconstruction_loss: 73.6470 - kl_loss: 8.4674\n",
      "Epoch 287/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.0095 - reconstruction_loss: 73.5884 - kl_loss: 8.4499\n",
      "Epoch 288/300\n",
      "57/57 [==============================] - 71s 1s/step - loss: 82.3790 - reconstruction_loss: 73.6417 - kl_loss: 8.4935\n",
      "Epoch 289/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.4786 - reconstruction_loss: 73.6390 - kl_loss: 8.4870\n",
      "Epoch 290/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.1526 - reconstruction_loss: 73.6908 - kl_loss: 8.4314\n",
      "Epoch 291/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.7553 - reconstruction_loss: 73.5771 - kl_loss: 8.4406\n",
      "Epoch 292/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.7351 - reconstruction_loss: 73.5588 - kl_loss: 8.4422\n",
      "Epoch 293/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.8435 - reconstruction_loss: 73.5800 - kl_loss: 8.4629\n",
      "Epoch 294/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.1930 - reconstruction_loss: 73.4997 - kl_loss: 8.4880\n",
      "Epoch 295/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.9410 - reconstruction_loss: 73.5440 - kl_loss: 8.4540\n",
      "Epoch 296/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.2571 - reconstruction_loss: 73.6699 - kl_loss: 8.4646\n",
      "Epoch 297/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.5361 - reconstruction_loss: 73.5130 - kl_loss: 8.4726\n",
      "Epoch 298/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 82.3998 - reconstruction_loss: 73.5273 - kl_loss: 8.5252\n",
      "Epoch 299/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.6538 - reconstruction_loss: 73.4853 - kl_loss: 8.4652\n",
      "Epoch 300/300\n",
      "57/57 [==============================] - 69s 1s/step - loss: 81.1658 - reconstruction_loss: 73.4698 - kl_loss: 8.4221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20656df9250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(X_train_final, epochs=300, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966332cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample1\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample2\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample4\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample7\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample8\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample10\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample11\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample12\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample13\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample14\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample15\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample16\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample20\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample21\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample22\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample23\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample25\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample29\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample30\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "(152, 92, 136, 1)\n",
      "(152, 92, 136)\n",
      "Sample1 is reconstrcting\n",
      "Sample2 is reconstrcting\n",
      "Sample4 is reconstrcting\n",
      "Sample7 is reconstrcting\n",
      "Sample8 is reconstrcting\n",
      "Sample10 is reconstrcting\n",
      "Sample11 is reconstrcting\n",
      "Sample12 is reconstrcting\n",
      "Sample13 is reconstrcting\n",
      "Sample14 is reconstrcting\n",
      "Sample15 is reconstrcting\n",
      "Sample16 is reconstrcting\n",
      "Sample20 is reconstrcting\n",
      "Sample21 is reconstrcting\n",
      "Sample22 is reconstrcting\n",
      "Sample23 is reconstrcting\n",
      "Sample25 is reconstrcting\n",
      "Sample29 is reconstrcting\n",
      "Sample30 is reconstrcting\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_test'\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_train'\n",
    "data_path = 'C:/Users/Administrator/Desktop/Dataset1_sample'\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset_noslice_sample'\n",
    "folder_list1 = os.listdir(data_path)\n",
    "folder_list = sorted(folder_list1, key=lambda x: int(x[6:]))\n",
    "for folder in folder_list:\n",
    "    print(folder)\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    file_list1 = os.listdir(folder_path)\n",
    "    file_list = sorted(file_list1, key=lambda info: (int(info[0:-4]), info[-4:]))\n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        img = cv2.imread(file_path)\n",
    "        pic = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        pic_list = pic.tolist()\n",
    "        X_test.append(pic_list)\n",
    "X_test_np = np.array(X_test)\n",
    "X_test_final = np.expand_dims(X_test_np, -1).astype(\"float32\") / 255\n",
    "#z_mean, _, _ = vae.encoder.predict(X_test_final)\n",
    "#x_decoded = vae.decoder.predict(z_mean)\n",
    "_, _, z_mean = vae.encoder.predict(X_test_final)\n",
    "x_decoded = vae.decoder.predict(z_mean)\n",
    "print(x_decoded.shape)\n",
    "x_decoded_reshape = x_decoded.reshape(152, 92, 136)*255\n",
    "#x_decoded_reshape = x_decoded.reshape(152, 56, 100)*255\n",
    "print(x_decoded_reshape.shape)\n",
    "\n",
    "reconstr_path = 'C:/Users/Administrator/Desktop/reconstruction/'\n",
    "os.mkdir(reconstr_path)\n",
    "for count in range(7296):\n",
    "#for count in range(152):\n",
    "    tmp = x_decoded_reshape[count,:,:]\n",
    "    count_int = count//8\n",
    "    #count_int_2 = count_int + 1\n",
    "    count_residue = count%8\n",
    "    count_residue_2 = count_residue + 1\n",
    "    count_residue_3 = count_residue_2 * 5\n",
    "    sample_name = folder_list[count_int]\n",
    "    #sample_name = 'Sample'+ str(count_int_2)\n",
    "    recon_file_path = os.path.join(reconstr_path, sample_name)\n",
    "    file_name = str(count_residue_3)+'.png'\n",
    "    recon_file_path2 = os.path.join(recon_file_path, file_name)\n",
    "    if count_residue == 0:\n",
    "        os.mkdir(recon_file_path)\n",
    "        print(sample_name, 'is reconstrcting')\n",
    "    cv2.imwrite(recon_file_path2,tmp)\n",
    "\n",
    "#X_test= []\n",
    "#test_file_path = 'C:/Users/Administrator/Desktop/Dataset1_test/Sample388/79.png'\n",
    "#img = cv2.imread(test_file_path)\n",
    "#pic = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "#pic_list = pic.tolist()\n",
    "#X_test.append(pic_list)\n",
    "#X_test_np = np.array(X_test)\n",
    "#X_test_final = np.expand_dims(X_test_np, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c738596b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "878fc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_display = z_mean[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2918c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_display.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f10d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('C:/Users/Administrator/Desktop/z_distribution.txt',z_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5f34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#def plot_label_clusters(vae, data):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    #z_mean, _, _ = vae.encoder.predict(data)\n",
    "    #plt.figure(figsize=(12, 10))\n",
    "   # plt.scatter(z_mean[:, 0], z_mean[:, 1])\n",
    "    #plt.xlabel(\"z[0]\")\n",
    "    #plt.ylabel(\"z[1]\")\n",
    "    #plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#plot_label_clusters(vae, X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f28c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reduce(path):\n",
    "    img = cv2.imread(path)\n",
    "    pic = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    pic_list = pic.tolist()\n",
    "    return pic_list\n",
    "\n",
    "train_X_pre = []\n",
    "train_Y_pre = []\n",
    "train_X = []\n",
    "train_Y = []\n",
    "#c_dir_path = 'C:/Users/Administrator/Desktop/Dataset1_train_vae'\n",
    "c_dir_path = 'C:/Users/Administrator/Desktop/Dataset_noslice_train_vae'\n",
    "#c_dir_path = 'C:/Users/Administrator/Desktop/Dataset1_sample_repeat'\n",
    "p_c_dir_list = os.listdir(c_dir_path)\n",
    "for p_c_dir in p_c_dir_list:\n",
    "    dir_path = os.path.join(c_dir_path, p_c_dir)\n",
    "    filelist1 = os.listdir(dir_path)\n",
    "    filelist = sorted(filelist1, key=lambda info: (int(info[0:-4]), info[-4:]))\n",
    "    print()\n",
    "    #for i in range(8):\n",
    "    for i in range(6):\n",
    "        train_X_pre.append(reduce(os.path.join(dir_path, filelist[i])))\n",
    "        train_X_pre.append(reduce(os.path.join(dir_path, filelist[i+1])))\n",
    "        train_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+2])))\n",
    "        train_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+3])))\n",
    "        train_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+4])))\n",
    "        train_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+5])))\n",
    "        train_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+6])))\n",
    "        train_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a01ac56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_pre_np = np.array(train_X_pre)\n",
    "train_Y_pre_np = np.array(train_Y_pre)\n",
    "train_X_unencode = np.expand_dims(train_X_pre_np, -1).astype(\"float32\") / 255\n",
    "train_Y_unencode = np.expand_dims(train_Y_pre_np, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f0c0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_np, _, _ = vae.encoder.predict(train_X_unencode)\n",
    "train_Y_np, _, _ = vae.encoder.predict(train_Y_unencode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9139027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2db80d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a7a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_np = train_X_np.reshape((90, 2, 100))\n",
    "train_Y_np = train_Y_np.reshape((90, 6, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a055e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('C:/Users/Administrator/Desktop/train_X_np.npy', train_X_np)\n",
    "#np.save('C:/Users/Administrator/Desktop/train_Y_np.npy', train_Y_np)\n",
    "np.save('C:/Users/Administrator/Desktop/train_X_np_noslice.npy', train_X_np)\n",
    "np.save('C:/Users/Administrator/Desktop/train_Y_np_noslice.npy', train_Y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7c13a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_pre = []\n",
    "test_Y_pre = []\n",
    "test_X = []\n",
    "test_Y = []\n",
    "#c_dir_path = 'C:/Users/Administrator/Desktop/Dataset_noslice_test_vae'\n",
    "c_dir_path = 'C:/Users/Administrator/Desktop/Dataset1_test_vae'\n",
    "p_c_dir_list1 = os.listdir(c_dir_path)\n",
    "p_c_dir_list = sorted(p_c_dir_list1, key=lambda x: int(x[6:]))\n",
    "for p_c_dir in p_c_dir_list:\n",
    "    dir_path = os.path.join(c_dir_path, p_c_dir)\n",
    "    filelist1 = os.listdir(dir_path)\n",
    "    filelist = sorted(filelist1, key=lambda info: (int(info[0:-4]), info[-4:]))\n",
    "    for i in range(6):\n",
    "        test_X_pre.append(reduce(os.path.join(dir_path, filelist[i])))\n",
    "        test_X_pre.append(reduce(os.path.join(dir_path, filelist[i+1])))\n",
    "        test_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+2])))\n",
    "        test_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+3])))\n",
    "        test_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+4])))\n",
    "        test_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+5])))\n",
    "        test_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+6])))\n",
    "        test_Y_pre.append(reduce(os.path.join(dir_path, filelist[i+7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e4e9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_pre_np = np.array(test_X_pre)\n",
    "test_Y_pre_np = np.array(test_Y_pre)\n",
    "test_X_unencode = np.expand_dims(test_X_pre_np, -1).astype(\"float32\") / 255\n",
    "test_Y_unencode = np.expand_dims(test_Y_pre_np, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "770e8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_np, _, _ = vae.encoder.predict(test_X_unencode)\n",
    "test_Y_np, _, _ = vae.encoder.predict(test_Y_unencode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6af9b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1344, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "256d0742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4032, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a1be019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_X_np = test_X_np.reshape((24, 2, 100))\n",
    "#test_Y_np = test_Y_np.reshape((24, 6, 100))\n",
    "test_X_np = test_X_np.reshape((672, 2, 100))\n",
    "test_Y_np = test_Y_np.reshape((672, 6, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ed7111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('C:/Users/Administrator/Desktop/test_X_np.npy', test_X_np)\n",
    "#np.save('C:/Users/Administrator/Desktop/test_Y_np.npy', test_Y_np)\n",
    "#np.save('C:/Users/Administrator/Desktop/test_X_np_noslice.npy', test_X_np)\n",
    "#np.save('C:/Users/Administrator/Desktop/test_Y_np_noslice.npy', test_Y_np)\n",
    "np.save('C:/Users/Administrator/Desktop/test_X_np_noslice_rare.npy', test_X_np)\n",
    "np.save('C:/Users/Administrator/Desktop/test_Y_np_noslice_rare.npy', test_Y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61b8123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.decoder.save('C:/Users/Administrator/Desktop/decoder.h5')\n",
    "vae.decoder.save('C:/Users/Administrator/Desktop/decoder_300_128.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39465e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.encoder.save('C:/Users/Administrator/Desktop/encoder.h5')\n",
    "vae.encoder.save('C:/Users/Administrator/Desktop/encoder_300_128.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd98fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample1\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample2\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample4\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample7\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample8\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample10\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample11\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample12\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample13\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample14\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample15\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample16\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample20\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample21\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample22\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "(120, 100)\n"
     ]
    }
   ],
   "source": [
    "train_X_life_pre = []\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_train_life'\n",
    "data_path = 'C:/Users/Administrator/Desktop/Dataset_noslice_train_life'\n",
    "folder_list1 = os.listdir(data_path)\n",
    "folder_list = sorted(folder_list1, key=lambda x: int(x[6:]))\n",
    "for folder in folder_list:\n",
    "    print(folder)\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    file_list1 = os.listdir(folder_path)\n",
    "    file_list = sorted(file_list1, key=lambda info: (int(info[0:-4]), info[-4:]))\n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        img = cv2.imread(file_path)\n",
    "        pic = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        pic_list = pic.tolist()\n",
    "        train_X_life_pre.append(pic_list)\n",
    "train_X_life_pre_np = np.array(train_X_life_pre)\n",
    "train_X_life_pre_final = np.expand_dims(train_X_life_pre_np, -1).astype(\"float32\") / 255\n",
    "z_mean_train, _, _ = vae.encoder.predict(train_X_life_pre_final)\n",
    "print(z_mean_train.shape)\n",
    "#x_decoded = vae.decoder.predict(z_mean)\n",
    "#print(x_decoded.shape)\n",
    "#x_decoded_reshape = x_decoded.reshape(8000, 92, 136)*255\n",
    "#print(x_decoded_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29734b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('C:/Users/Administrator/Desktop/train_X_life_np.npy', z_mean_train)\n",
    "np.save('C:/Users/Administrator/Desktop/train_X_life_np_noslice.npy', z_mean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48808b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample23\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample25\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample29\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "Sample30\n",
      "5.png\n",
      "10.png\n",
      "15.png\n",
      "20.png\n",
      "25.png\n",
      "30.png\n",
      "35.png\n",
      "40.png\n",
      "(32, 100)\n"
     ]
    }
   ],
   "source": [
    "test_X_life_pre = []\n",
    "#data_path = 'C:/Users/Administrator/Desktop/Dataset1_test_life'\n",
    "data_path = 'C:/Users/Administrator/Desktop/Dataset_noslice_test_life'\n",
    "folder_list1 = os.listdir(data_path)\n",
    "folder_list = sorted(folder_list1, key=lambda x: int(x[6:]))\n",
    "for folder in folder_list:\n",
    "    print(folder)\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    file_list1 = os.listdir(folder_path)\n",
    "    file_list = sorted(file_list1, key=lambda info: (int(info[0:-4]), info[-4:]))\n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        img = cv2.imread(file_path)\n",
    "        pic = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        pic_list = pic.tolist()\n",
    "        test_X_life_pre.append(pic_list)\n",
    "test_X_life_pre_np = np.array(test_X_life_pre)\n",
    "test_X_life_pre_final = np.expand_dims(test_X_life_pre_np, -1).astype(\"float32\") / 255\n",
    "z_mean_test, _, _ = vae.encoder.predict(test_X_life_pre_final)\n",
    "print(z_mean_test.shape)\n",
    "#x_decoded = vae.decoder.predict(z_mean)\n",
    "#print(x_decoded.shape)\n",
    "#x_decoded_reshape = x_decoded.reshape(8000, 92, 136)*255\n",
    "#print(x_decoded_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f88dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('C:/Users/Administrator/Desktop/test_X_life_np.npy', z_mean_test)\n",
    "np.save('C:/Users/Administrator/Desktop/test_X_life_np_noslice.npy', z_mean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7d74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
